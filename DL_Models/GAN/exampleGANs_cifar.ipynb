{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T03:36:48.362174Z",
     "start_time": "2018-12-11T03:35:00.880932Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#you need to run the cifar10.py and dataset.py before importing them\n",
    "import cifar10\n",
    "\n",
    "#make sure to change it to your own path\n",
    "\n",
    "#cifar10.data_path = \"C:/Users/Ali/data/CIFAR-10\"\n",
    "#cifar10.data_path = \"D:/WORK/Columbia/Courses/DeepLearning/CA/Code/Week 5\"\n",
    "cifar10.data_path = \"./\"\n",
    "cifar10.maybe_download_and_extract()\n",
    "\n",
    "\n",
    "class_names = cifar10.load_class_names()\n",
    "class_names\n",
    "\n",
    "images_train, cls_train, labels_train = cifar10.load_training_data()\n",
    "\n",
    "images_test, cls_test, labels_test = cifar10.load_test_data()\n",
    "\n",
    "print(\"Size of:\")\n",
    "print(\"- Training-set:\\t\\t{}\".format(len(images_train)))\n",
    "print(\"- Test-set:\\t\\t{}\".format(len(images_test)))\n",
    "\n",
    "from cifar10 import img_size, num_channels, num_classes\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T03:43:57.817911Z",
     "start_time": "2018-12-11T03:43:57.736829Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CIFAR data image of shape 32*32*3=3072\n",
    "#CIFAR_size = 32*32*3\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3072])\n",
    "Z = tf.placeholder(tf.float32, shape=[None, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up parameters for generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T03:43:58.386340Z",
     "start_time": "2018-12-11T03:43:58.160243Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generator\n",
    "# Define the variables for the generator, we will use them to build layers later\n",
    "# -------------------\n",
    "size_g_w1 = 100\n",
    "size_g_b1 = 512\n",
    "# A good way to decide the std for initializing the weights\n",
    "w1_std = 1.0/tf.sqrt(size_g_w1/2.0)\n",
    "\n",
    "G_W1 = tf.Variable(tf.random_normal(shape=[size_g_w1, size_g_b1], stddev=w1_std))\n",
    "G_b1 = tf.Variable(tf.zeros(shape=[size_g_b1]))\n",
    "\n",
    "size_g_w2 = 512\n",
    "size_g_b2 = 3072\n",
    "w2_std = 1.0/tf.sqrt(size_g_w2/2.0)\n",
    "\n",
    "G_W2 = tf.Variable(tf.random_normal(shape=[size_g_w2, size_g_b2], stddev=w2_std))\n",
    "G_b2 = tf.Variable(tf.zeros(shape=[size_g_b2]))\n",
    "# theta_G and theta_D will be feeded to different optimizers later as \"var_list\", \n",
    "# since currently we have two networks instead of one now.\n",
    "theta_G = [G_W1, G_W2, G_b1, G_b2]\n",
    "\n",
    "# ====================\n",
    "# Discriminator\n",
    "# Define the variables for the discriminator\n",
    "# --------------------\n",
    "size_d_w1 = 3072\n",
    "size_d_b1 = 512\n",
    "w1_std = 1.0/tf.sqrt(size_d_w1/2.0)\n",
    "\n",
    "D_W1 = tf.Variable(tf.random_normal(shape=[size_d_w1,size_d_b1], stddev=w1_std))\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[size_d_b1]))\n",
    "\n",
    "size_d_w2 = 512\n",
    "size_d_b2 = 1\n",
    "w2_std = 1.0/tf.sqrt(size_d_w2/2.0)\n",
    "\n",
    "D_W2 = tf.Variable(tf.random_normal(shape=[size_d_w2,size_d_b2], stddev=w2_std))\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[size_d_b2]))\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_b1, D_b2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T03:43:58.893933Z",
     "start_time": "2018-12-11T03:43:58.883749Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z):\n",
    "\n",
    "    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n",
    "    G_logit = tf.matmul(G_h1, G_W2) + G_b2\n",
    "    G_prob = tf.nn.sigmoid(G_logit)\n",
    "\n",
    "    return G_prob, G_logit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T03:44:00.006536Z",
     "start_time": "2018-12-11T03:43:59.998146Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(x):\n",
    "\n",
    "    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n",
    "    D_logit = tf.matmul(D_h1, D_W2) + D_b2\n",
    "    D_prob = tf.nn.sigmoid(D_logit)\n",
    "\n",
    "    return D_prob, D_logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate samples function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T03:44:01.081943Z",
     "start_time": "2018-12-11T03:44:01.078026Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_z(m, n):\n",
    "    # randomly generate samples for generator\n",
    "    return np.random.uniform(-1.0, 1.0, size = [m, n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T03:44:02.009856Z",
     "start_time": "2018-12-11T03:44:01.990399Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_sample(samples, size1, size2):\n",
    "    \n",
    "    fig1 = plt.figure(figsize=(size1, size2))\n",
    "    gs = gridspec.GridSpec(size1, size2)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        sample_reshaped=sample.reshape(32,32,3)\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample_reshaped[:,:,0], cmap='gray')\n",
    "        plt.imshow(sample_reshaped[:,:,1], cmap='gray')\n",
    "        plt.imshow(sample_reshaped[:,:,2], cmap='gray')\n",
    "\n",
    "    return fig1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faciliate the path defining process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T03:44:07.984901Z",
     "start_time": "2018-12-11T03:44:07.977698Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Though it's not possible to get the path to the notebook by __file__, os.path is still very useful in dealing with paths and files\n",
    "# In this case, we can use an alternative: pathlib.Path\n",
    "\"\"\"\n",
    "code_dir   = os.path.dirname(__file__)\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "#get the current path of our code\n",
    "code_dir = Path().resolve()\n",
    "#create output_dir within the same path\n",
    "output_dir = os.path.join(code_dir, 'outputGANs/')\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build GNN with defined vars and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T03:44:11.381835Z",
     "start_time": "2018-12-11T03:44:10.673336Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put randomly generated sample Z into the generator to create \"fake\" images\n",
    "G_sample, _ = generator(Z)\n",
    "# The result of discriminator of real and fake samples\n",
    "_, D_logit_real = discriminator(X)\n",
    "_, D_logit_fake = discriminator(G_sample)\n",
    "\n",
    "# generator loss \n",
    "# the goal of generator is to let discriminator make more mistakes on fake samples\n",
    "# tf.ones_like returns a tensor with all elements set to 1\n",
    "# 0 represent fake and 1 means real\n",
    "G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake)))\n",
    "G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n",
    "\n",
    "# discriminator loss \n",
    "D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))\n",
    "D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\n",
    "D_loss = D_loss_real + D_loss_fake\n",
    "D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T03:44:11.885713Z",
     "start_time": "2018-12-11T03:44:11.879432Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(\n",
    "    batch_size = 5,\n",
    "    from_test=False\n",
    "):\n",
    "    if from_test:\n",
    "        return images_test,labels_test\n",
    "    else:\n",
    "        index = np.random.choice(list(range(len(images_train))),size=batch_size)\n",
    "        return images_train[index], labels_train[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T03:50:44.091890Z",
     "start_time": "2018-12-11T03:44:13.192614Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batch_size = 128\n",
    "# the dimension of the random samples\n",
    "z_dim = 100\n",
    "result_freq = 1000\n",
    "# plot generators' output every figure_iter step\n",
    "figure_iter = 1000\n",
    "max_iter = 1000000\n",
    "size1 = 5\n",
    "size2 = 5\n",
    "i = 0\n",
    "\n",
    "for iter in range(max_iter):\n",
    "    \n",
    "    if iter % figure_iter == 0:\n",
    "        \n",
    "        # G_sample is a sample from the generator\n",
    "        samples = sess.run(G_sample, feed_dict={Z: sample_z(size1*size2, z_dim)})\n",
    "\n",
    "        fig1 = plot_sample(samples, size1, size2)\n",
    "        plt.savefig(output_dir + 'GANs' + str(i) + '.png', bbox_inches='tight')\n",
    "        i += 1\n",
    "        plt.close(fig1)\n",
    "\n",
    "    batch_xs, _ = next_batch(batch_size)\n",
    "    batch_xs=batch_xs.reshape(-1,3072)\n",
    "\n",
    "    _, discriminator_loss = sess.run([D_solver, D_loss], feed_dict={X: batch_xs, Z: sample_z(batch_size, z_dim)})\n",
    "    _, generator_loss     = sess.run([G_solver, G_loss], feed_dict={Z: sample_z(batch_size, z_dim)})\n",
    "\n",
    "    if iter % result_freq == 0:\n",
    "        print('iteration: {}'.format(iter))\n",
    "        print('D_loss: {:0.4}'.format(discriminator_loss))\n",
    "        print('G_loss: {:0.4}'.format(generator_loss))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
