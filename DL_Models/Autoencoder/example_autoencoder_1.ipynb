{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T12:44:09.131674Z",
     "start_time": "2018-10-02T12:44:09.129175Z"
    }
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T22:05:26.310842Z",
     "start_time": "2018-10-03T22:05:10.381461Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import argparse\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of the Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T22:05:26.324885Z",
     "start_time": "2018-10-03T22:05:26.313169Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Following Hinton-Salakhutdinov Architecture\n",
    "\n",
    "# 3 hidden layers for encoder\n",
    "n_encoder_h_1 = 1000\n",
    "n_encoder_h_2 = 500\n",
    "n_encoder_h_3 = 250\n",
    "\n",
    "# 3 hidden layers for decoder\n",
    "n_decoder_h_1 = 250\n",
    "n_decoder_h_2 = 500\n",
    "n_decoder_h_3 = 1000\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 200\n",
    "batch_size = 100\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T22:05:26.435086Z",
     "start_time": "2018-10-03T22:05:26.382374Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layer_batch_normalization(x, n_out, phase_train):\n",
    "    \"\"\"\n",
    "    Defines the network layers\n",
    "    input:\n",
    "        - x: input vector of the layer\n",
    "        - n_out: integer, depth of input maps - number of sample in the batch \n",
    "        - phase_train: boolean tf.Varialbe, true indicates training phase\n",
    "    output:\n",
    "        - batch-normalized maps   \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    beta_init = tf.constant_initializer(value=0.0, dtype=tf.float32)\n",
    "    beta = tf.get_variable(\"beta\", [n_out], initializer=beta_init)\n",
    "    \n",
    "    gamma_init = tf.constant_initializer(value=1.0, dtype=tf.float32)\n",
    "    gamma = tf.get_variable(\"gamma\", [n_out], initializer=gamma_init)\n",
    "\n",
    "    #tf.nn.moment: https://www.tensorflow.org/api_docs/python/tf/nn/moments\n",
    "    #calculate mean and variance of x\n",
    "    batch_mean, batch_var = tf.nn.moments(x, [0], name='moments')\n",
    "    \n",
    "    #tf.train.ExponentialMovingAverage:\n",
    "    #https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\n",
    "    #Maintains moving averages of variables by employing an exponential decay.\n",
    "    ema = tf.train.ExponentialMovingAverage(decay=0.9)\n",
    "    ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "    ema_mean, ema_var = ema.average(batch_mean), ema.average(batch_var)\n",
    "    \n",
    "    def mean_var_with_update():\n",
    "        with tf.control_dependencies([ema_apply_op]):\n",
    "            return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "        \n",
    "    #tf.cond: https://www.tensorflow.org/api_docs/python/tf/cond\n",
    "    #Return true_fn() if the predicate pred is true else false_fn()\n",
    "    mean, var = tf.cond(phase_train, mean_var_with_update, lambda: (ema_mean, ema_var))\n",
    "\n",
    "    reshaped_x = tf.reshape(x, [-1, 1, 1, n_out])\n",
    "    normed = tf.nn.batch_norm_with_global_normalization(reshaped_x, mean, var, beta, gamma, 1e-3, True)\n",
    "    \n",
    "    return tf.reshape(normed, [-1, n_out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of the Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T22:05:26.461121Z",
     "start_time": "2018-10-03T22:05:26.444649Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layer(x, weight_shape, bias_shape, phase_train):\n",
    "    \n",
    "    \"\"\"\n",
    "    Defines the network layers\n",
    "    input:\n",
    "        - x: input vector of the layer\n",
    "        - weight_shape: shape the the weight maxtrix\n",
    "        - bias_shape: shape of the bias vector\n",
    "        - phase_train: boolean tf.Varialbe, true indicates training phase\n",
    "    output:\n",
    "        - output vector of the layer after the matrix multiplication and non linear transformation\n",
    "    \"\"\"\n",
    "    \n",
    "    #initialize weights\n",
    "    weight_init = tf.random_normal_initializer(stddev=(1.0/weight_shape[0])**0.5)\n",
    "    W = tf.get_variable(\"W\", weight_shape, initializer=weight_init)\n",
    "    \n",
    "    bias_init = tf.constant_initializer(value=0)\n",
    "    b = tf.get_variable(\"b\", bias_shape, initializer=bias_init)\n",
    "\n",
    "    logits = tf.matmul(x, W) + b\n",
    "    \n",
    "    #apply the non-linear function after the batch normalization\n",
    "    return tf.nn.sigmoid(layer_batch_normalization(logits, weight_shape[1], phase_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T13:39:04.039484Z",
     "start_time": "2018-10-02T13:39:04.036698Z"
    }
   },
   "source": [
    "# Definition of the Encoder Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T22:05:26.484126Z",
     "start_time": "2018-10-03T22:05:26.463872Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoder(x, n_code, phase_train):\n",
    "    \n",
    "    \"\"\"\n",
    "    Defines the network encoder part\n",
    "    input:\n",
    "        - x: input vector of the encoder\n",
    "        - n_code: number of neurons in the code layer (output of the encoder - input of the decoder)\n",
    "        - phase_train: boolean tf.Varialbe, true indicates training phase\n",
    "    output:\n",
    "        - output vector: reduced dimension\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.variable_scope(\"encoder\"):\n",
    "        \n",
    "        with tf.variable_scope(\"h_1\"):\n",
    "            h_1 = layer(x, [784, n_encoder_h_1], [n_encoder_h_1], phase_train)\n",
    "\n",
    "        with tf.variable_scope(\"h_2\"):\n",
    "            h_2 = layer(h_1, [n_encoder_h_1, n_encoder_h_2], [n_encoder_h_2], phase_train)\n",
    "\n",
    "        with tf.variable_scope(\"h_3\"):\n",
    "            h_3 = layer(h_2, [n_encoder_h_2, n_encoder_h_3], [n_encoder_h_3], phase_train)\n",
    "\n",
    "        with tf.variable_scope(\"code\"):\n",
    "            output = layer(h_3, [n_encoder_h_3, n_code], [n_code], phase_train)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T13:39:56.962874Z",
     "start_time": "2018-10-02T13:39:56.960040Z"
    }
   },
   "source": [
    "# Definition of the Decoder Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T22:05:26.504557Z",
     "start_time": "2018-10-03T22:05:26.486050Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoder(x, n_code, phase_train):\n",
    "    \"\"\"\n",
    "    Defines the network encoder part\n",
    "    input:\n",
    "        - x: input vector of the decoder - reduced dimension vector\n",
    "        - n_code: number of neurons in the code layer (output of the encoder - input of the decoder) \n",
    "        - phase_train: boolean tf.Variablle, true indicates training phase\n",
    "    output:\n",
    "        - output vector: reconstructed dimension of the initial vector\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.variable_scope(\"decoder\"):\n",
    "        \n",
    "        with tf.variable_scope(\"h_1\"):\n",
    "            h_1 = layer(x, [n_code, n_decoder_h_1], [n_decoder_h_1], phase_train)\n",
    "\n",
    "        with tf.variable_scope(\"h_2\"):\n",
    "            h_2 = layer(h_1, [n_decoder_h_1, n_decoder_h_2], [n_decoder_h_2], phase_train)\n",
    "\n",
    "        with tf.variable_scope(\"h_3\"):\n",
    "            h_3 = layer(h_2, [n_decoder_h_2, n_decoder_h_3], [n_decoder_h_3], phase_train)\n",
    "\n",
    "        with tf.variable_scope(\"output\"):\n",
    "            output = layer(h_3, [n_decoder_h_3, 784], [784], phase_train)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T13:40:11.932837Z",
     "start_time": "2018-10-02T13:40:11.929439Z"
    }
   },
   "source": [
    "# Definition of the Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T22:05:26.517979Z",
     "start_time": "2018-10-03T22:05:26.506902Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(output, x):\n",
    "    \"\"\"\n",
    "    Compute the loss of the auto-encoder\n",
    "    \n",
    "    intput:\n",
    "        - output: the output of the decoder\n",
    "        - x: true value of the sample batch - this is the input of the encoder\n",
    "        \n",
    "        the two have the same shape (batch_size * num_of_classes)\n",
    "    output:\n",
    "        - loss: loss of the corresponding batch (scalar tensor)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.variable_scope(\"training\"):\n",
    "        \n",
    "        l2 = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(output, x)), 1))\n",
    "        train_loss = tf.reduce_mean(l2)\n",
    "        train_summary_op = tf.summary.scalar(\"train_cost\", train_loss)\n",
    "        return train_loss, train_summary_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T22:05:26.531705Z",
     "start_time": "2018-10-03T22:05:26.520900Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training(cost, global_step):\n",
    "    \"\"\"\n",
    "    defines the necessary elements to train the network\n",
    "    \n",
    "    intput:\n",
    "        - cost: the cost is the loss of the corresponding batch\n",
    "        - global_step: number of batch seen so far, it is incremented by one \n",
    "        each time the .minimize() function is called\n",
    "    \"\"\"\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name='Adam')\n",
    "    train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T22:05:26.558495Z",
     "start_time": "2018-10-03T22:05:26.534046Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(output, x):\n",
    "    \"\"\"\n",
    "    evaluates the accuracy on the validation set \n",
    "    input:\n",
    "        -output: prediction vector of the network for the validation set\n",
    "        -x: true value for the validation set\n",
    "    output:\n",
    "        - val_loss: loss of the autoencoder\n",
    "        - in_image_op: input image \n",
    "        - out_image_op:reconstructed image \n",
    "        - val_summary_op: summary of the loss\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.variable_scope(\"validation\"):\n",
    "        \n",
    "        in_image_op = image_summary(\"input_image\", x)\n",
    "        \n",
    "        out_image_op = image_summary(\"output_image\", output)\n",
    "        \n",
    "        l2_norm = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(output, x, name=\"val_diff\")), 1))\n",
    "        \n",
    "        val_loss = tf.reduce_mean(l2_norm)\n",
    "        \n",
    "        val_summary_op = tf.summary.scalar(\"val_cost\", val_loss)\n",
    "        \n",
    "        return val_loss, in_image_op, out_image_op, val_summary_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T13:41:48.805072Z",
     "start_time": "2018-10-02T13:41:48.802014Z"
    }
   },
   "source": [
    "# Image Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T22:05:26.571265Z",
     "start_time": "2018-10-03T22:05:26.564897Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def image_summary(label, tensor):\n",
    "    #tf.summary.image: https://www.tensorflow.org/api_docs/python/tf/summary/image\n",
    "    #Outputs a Summary protocol buffer with images.\n",
    "\n",
    "    tensor_reshaped = tf.reshape(tensor, [-1, 28, 28, 1])\n",
    "    return tf.summary.image(label, tensor_reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T13:51:07.293398Z",
     "start_time": "2018-10-02T13:51:07.282274Z"
    }
   },
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T22:07:06.083741Z",
     "start_time": "2018-10-03T22:05:26.575512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 11.564421090\n",
      "Validation Loss: 11.725205\n",
      "Epoch: 0002 cost = 9.381576670\n",
      "Validation Loss: 8.61294\n",
      "Epoch: 0003 cost = 8.390918994\n",
      "Validation Loss: 7.376416\n",
      "Epoch: 0004 cost = 7.775207559\n",
      "Validation Loss: 7.260479\n",
      "Epoch: 0005 cost = 7.432604048\n",
      "Validation Loss: 7.629019\n",
      "Epoch: 0006 cost = 7.104031122\n",
      "Validation Loss: 6.881897\n",
      "Epoch: 0007 cost = 6.865076444\n",
      "Validation Loss: 6.4956985\n",
      "Epoch: 0008 cost = 6.711924877\n",
      "Validation Loss: 6.477389\n",
      "Epoch: 0009 cost = 6.553516642\n",
      "Validation Loss: 6.2858257\n",
      "Epoch: 0010 cost = 6.420095656\n",
      "Validation Loss: 6.2364507\n",
      "Epoch: 0011 cost = 6.274219853\n",
      "Validation Loss: 6.032448\n",
      "Epoch: 0012 cost = 6.063214063\n",
      "Validation Loss: 5.946652\n",
      "Epoch: 0013 cost = 5.995332215\n",
      "Validation Loss: 5.9253306\n",
      "Epoch: 0014 cost = 5.937842406\n",
      "Validation Loss: 5.8148055\n",
      "Epoch: 0015 cost = 5.883486397\n",
      "Validation Loss: 5.7961135\n",
      "Epoch: 0016 cost = 5.842212829\n",
      "Validation Loss: 5.7508264\n",
      "Epoch: 0017 cost = 5.807370794\n",
      "Validation Loss: 5.6705456\n",
      "Epoch: 0018 cost = 5.776732720\n",
      "Validation Loss: 5.67806\n",
      "Epoch: 0019 cost = 5.750384330\n",
      "Validation Loss: 5.6573277\n",
      "Epoch: 0020 cost = 5.721517105\n",
      "Validation Loss: 5.5735726\n",
      "Epoch: 0021 cost = 5.696260587\n",
      "Validation Loss: 5.574943\n",
      "Epoch: 0022 cost = 5.678179524\n",
      "Validation Loss: 5.5603437\n",
      "Epoch: 0023 cost = 5.656422734\n",
      "Validation Loss: 5.5533476\n",
      "Epoch: 0024 cost = 5.636962983\n",
      "Validation Loss: 5.500772\n",
      "Epoch: 0025 cost = 5.620416928\n",
      "Validation Loss: 5.4600096\n",
      "Epoch: 0026 cost = 5.596686704\n",
      "Validation Loss: 5.4681783\n",
      "Epoch: 0027 cost = 5.584507486\n",
      "Validation Loss: 5.4946384\n",
      "Epoch: 0028 cost = 5.564632808\n",
      "Validation Loss: 5.432046\n",
      "Epoch: 0029 cost = 5.547310087\n",
      "Validation Loss: 5.408332\n",
      "Epoch: 0030 cost = 5.534173555\n",
      "Validation Loss: 5.4322076\n",
      "Epoch: 0031 cost = 5.527596080\n",
      "Validation Loss: 5.355286\n",
      "Epoch: 0032 cost = 5.510770768\n",
      "Validation Loss: 5.3426366\n",
      "Epoch: 0033 cost = 5.498935646\n",
      "Validation Loss: 5.3680224\n",
      "Epoch: 0034 cost = 5.481639432\n",
      "Validation Loss: 5.305327\n",
      "Epoch: 0035 cost = 5.472129374\n",
      "Validation Loss: 5.3596435\n",
      "Epoch: 0036 cost = 5.465673800\n",
      "Validation Loss: 5.2729316\n",
      "Epoch: 0037 cost = 5.446024303\n",
      "Validation Loss: 5.294128\n",
      "Epoch: 0038 cost = 5.434395120\n",
      "Validation Loss: 5.269825\n",
      "Epoch: 0039 cost = 5.431583333\n",
      "Validation Loss: 5.2666664\n",
      "Epoch: 0040 cost = 5.421595037\n",
      "Validation Loss: 5.261152\n",
      "Epoch: 0041 cost = 5.411421185\n",
      "Validation Loss: 5.2602053\n",
      "Epoch: 0042 cost = 5.401717029\n",
      "Validation Loss: 5.2116213\n",
      "Epoch: 0043 cost = 5.391168573\n",
      "Validation Loss: 5.2097697\n",
      "Epoch: 0044 cost = 5.384582625\n",
      "Validation Loss: 5.219593\n",
      "Epoch: 0045 cost = 5.379067896\n",
      "Validation Loss: 5.2155957\n",
      "Epoch: 0046 cost = 5.364964314\n",
      "Validation Loss: 5.2284255\n",
      "Epoch: 0047 cost = 5.358037571\n",
      "Validation Loss: 5.2049494\n",
      "Epoch: 0048 cost = 5.351915061\n",
      "Validation Loss: 5.1893096\n",
      "Epoch: 0049 cost = 5.344158748\n",
      "Validation Loss: 5.2217326\n",
      "Epoch: 0050 cost = 5.337751586\n",
      "Validation Loss: 5.157026\n",
      "Epoch: 0051 cost = 5.328929475\n",
      "Validation Loss: 5.157149\n",
      "Epoch: 0052 cost = 5.321411462\n",
      "Validation Loss: 5.168374\n",
      "Epoch: 0053 cost = 5.314725151\n",
      "Validation Loss: 5.153708\n",
      "Epoch: 0054 cost = 5.303274120\n",
      "Validation Loss: 5.153466\n",
      "Epoch: 0055 cost = 5.301484823\n",
      "Validation Loss: 5.1506577\n",
      "Epoch: 0056 cost = 5.297081511\n",
      "Validation Loss: 5.1543374\n",
      "Epoch: 0057 cost = 5.286930241\n",
      "Validation Loss: 5.1219296\n",
      "Epoch: 0058 cost = 5.285850235\n",
      "Validation Loss: 5.134939\n",
      "Epoch: 0059 cost = 5.277355596\n",
      "Validation Loss: 5.105132\n",
      "Epoch: 0060 cost = 5.273740376\n",
      "Validation Loss: 5.1104116\n",
      "Epoch: 0061 cost = 5.268735685\n",
      "Validation Loss: 5.1033587\n",
      "Epoch: 0062 cost = 5.264020764\n",
      "Validation Loss: 5.105149\n",
      "Epoch: 0063 cost = 5.251184471\n",
      "Validation Loss: 5.1418037\n",
      "Epoch: 0064 cost = 5.249152483\n",
      "Validation Loss: 5.112163\n",
      "Epoch: 0065 cost = 5.243313768\n",
      "Validation Loss: 5.106496\n",
      "Epoch: 0066 cost = 5.234590854\n",
      "Validation Loss: 5.104955\n",
      "Epoch: 0067 cost = 5.226164928\n",
      "Validation Loss: 5.064046\n",
      "Epoch: 0068 cost = 5.233390032\n",
      "Validation Loss: 5.0703497\n",
      "Epoch: 0069 cost = 5.226116838\n",
      "Validation Loss: 5.0741553\n",
      "Epoch: 0070 cost = 5.216792360\n",
      "Validation Loss: 5.059868\n",
      "Epoch: 0071 cost = 5.216796406\n",
      "Validation Loss: 5.056584\n",
      "Epoch: 0072 cost = 5.209933155\n",
      "Validation Loss: 5.0751133\n",
      "Epoch: 0073 cost = 5.207831880\n",
      "Validation Loss: 5.0486007\n",
      "Epoch: 0074 cost = 5.203569808\n",
      "Validation Loss: 5.1030555\n",
      "Epoch: 0075 cost = 5.195006965\n",
      "Validation Loss: 5.099019\n",
      "Epoch: 0076 cost = 5.188758945\n",
      "Validation Loss: 5.0283923\n",
      "Epoch: 0077 cost = 5.179369038\n",
      "Validation Loss: 5.0294256\n",
      "Epoch: 0078 cost = 5.181370954\n",
      "Validation Loss: 5.0566435\n",
      "Epoch: 0079 cost = 5.178742782\n",
      "Validation Loss: 5.0915446\n",
      "Epoch: 0080 cost = 5.169644241\n",
      "Validation Loss: 5.0483537\n",
      "Epoch: 0081 cost = 5.169297275\n",
      "Validation Loss: 5.0122657\n",
      "Epoch: 0082 cost = 5.167539089\n",
      "Validation Loss: 5.064414\n",
      "Epoch: 0083 cost = 5.160867229\n",
      "Validation Loss: 4.9952607\n",
      "Epoch: 0084 cost = 5.153092716\n",
      "Validation Loss: 5.0569477\n",
      "Epoch: 0085 cost = 5.149797822\n",
      "Validation Loss: 5.0190587\n",
      "Epoch: 0086 cost = 5.156326238\n",
      "Validation Loss: 5.0074973\n",
      "Epoch: 0087 cost = 5.144202661\n",
      "Validation Loss: 4.987751\n",
      "Epoch: 0088 cost = 5.133471947\n",
      "Validation Loss: 5.002239\n",
      "Epoch: 0089 cost = 5.132918279\n",
      "Validation Loss: 4.9917984\n",
      "Epoch: 0090 cost = 5.136075767\n",
      "Validation Loss: 5.0102744\n",
      "Epoch: 0091 cost = 5.126501768\n",
      "Validation Loss: 5.0437717\n",
      "Epoch: 0092 cost = 5.125658352\n",
      "Validation Loss: 4.994284\n",
      "Epoch: 0093 cost = 5.118465566\n",
      "Validation Loss: 4.986377\n",
      "Epoch: 0094 cost = 5.119839067\n",
      "Validation Loss: 5.0004034\n",
      "Epoch: 0095 cost = 5.122217029\n",
      "Validation Loss: 4.9680376\n",
      "Epoch: 0096 cost = 5.110703474\n",
      "Validation Loss: 4.99678\n",
      "Epoch: 0097 cost = 5.111409747\n",
      "Validation Loss: 4.987544\n",
      "Epoch: 0098 cost = 5.110172479\n",
      "Validation Loss: 5.0253415\n",
      "Epoch: 0099 cost = 5.109316711\n",
      "Validation Loss: 4.9891353\n",
      "Epoch: 0100 cost = 5.103111519\n",
      "Validation Loss: 4.999018\n",
      "Epoch: 0101 cost = 5.101874118\n",
      "Validation Loss: 4.975948\n",
      "Epoch: 0102 cost = 5.098800794\n",
      "Validation Loss: 5.014597\n",
      "Epoch: 0103 cost = 5.095126255\n",
      "Validation Loss: 5.02071\n",
      "Epoch: 0104 cost = 5.084271845\n",
      "Validation Loss: 4.9697156\n",
      "Epoch: 0105 cost = 5.086092041\n",
      "Validation Loss: 4.9694657\n",
      "Epoch: 0106 cost = 5.090585875\n",
      "Validation Loss: 4.9788237\n",
      "Epoch: 0107 cost = 5.082387459\n",
      "Validation Loss: 4.9828663\n",
      "Epoch: 0108 cost = 5.078115505\n",
      "Validation Loss: 4.9695234\n",
      "Epoch: 0109 cost = 5.082462195\n",
      "Validation Loss: 4.9586353\n",
      "Epoch: 0110 cost = 5.075285177\n",
      "Validation Loss: 4.962438\n",
      "Epoch: 0111 cost = 5.072702860\n",
      "Validation Loss: 4.9714603\n",
      "Epoch: 0112 cost = 5.065692197\n",
      "Validation Loss: 4.9728346\n",
      "Epoch: 0113 cost = 5.066605682\n",
      "Validation Loss: 4.959972\n",
      "Epoch: 0114 cost = 5.063177280\n",
      "Validation Loss: 4.9819107\n",
      "Epoch: 0115 cost = 5.068051286\n",
      "Validation Loss: 4.9595447\n",
      "Epoch: 0116 cost = 5.056093884\n",
      "Validation Loss: 4.9297423\n",
      "Epoch: 0117 cost = 5.054996888\n",
      "Validation Loss: 4.9666295\n",
      "Epoch: 0118 cost = 5.057748122\n",
      "Validation Loss: 4.9732018\n",
      "Epoch: 0119 cost = 5.052603101\n",
      "Validation Loss: 4.9718027\n",
      "Epoch: 0120 cost = 5.053419352\n",
      "Validation Loss: 4.944764\n",
      "Epoch: 0121 cost = 5.050195990\n",
      "Validation Loss: 4.9776297\n",
      "Epoch: 0122 cost = 5.051139976\n",
      "Validation Loss: 4.9770594\n",
      "Epoch: 0123 cost = 5.044401863\n",
      "Validation Loss: 4.932683\n",
      "Epoch: 0124 cost = 5.050007256\n",
      "Validation Loss: 5.044132\n",
      "Epoch: 0125 cost = 5.038133279\n",
      "Validation Loss: 4.941375\n",
      "Epoch: 0126 cost = 5.041862972\n",
      "Validation Loss: 4.942138\n",
      "Epoch: 0127 cost = 5.041355775\n",
      "Validation Loss: 4.906914\n",
      "Epoch: 0128 cost = 5.043204885\n",
      "Validation Loss: 4.9416876\n",
      "Epoch: 0129 cost = 5.035888937\n",
      "Validation Loss: 4.9394116\n",
      "Epoch: 0130 cost = 5.029956324\n",
      "Validation Loss: 4.9353633\n",
      "Epoch: 0131 cost = 5.032598035\n",
      "Validation Loss: 4.9272323\n",
      "Epoch: 0132 cost = 5.029061269\n",
      "Validation Loss: 4.911135\n",
      "Epoch: 0133 cost = 5.024242986\n",
      "Validation Loss: 4.9588103\n",
      "Epoch: 0134 cost = 5.024229003\n",
      "Validation Loss: 4.932043\n",
      "Epoch: 0135 cost = 5.020637628\n",
      "Validation Loss: 4.9136376\n",
      "Epoch: 0136 cost = 5.016512554\n",
      "Validation Loss: 4.9206476\n",
      "Epoch: 0137 cost = 5.017605799\n",
      "Validation Loss: 4.921246\n",
      "Epoch: 0138 cost = 5.018513391\n",
      "Validation Loss: 4.948062\n",
      "Epoch: 0139 cost = 5.008988104\n",
      "Validation Loss: 4.9446573\n",
      "Epoch: 0140 cost = 5.008754168\n",
      "Validation Loss: 4.9285393\n",
      "Epoch: 0141 cost = 5.008969112\n",
      "Validation Loss: 4.8934665\n",
      "Epoch: 0142 cost = 5.008802444\n",
      "Validation Loss: 4.910892\n",
      "Epoch: 0143 cost = 5.011365200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4.925441\n",
      "Epoch: 0144 cost = 5.007147345\n",
      "Validation Loss: 4.918103\n",
      "Epoch: 0145 cost = 5.001471501\n",
      "Validation Loss: 4.9182243\n",
      "Epoch: 0146 cost = 5.001704925\n",
      "Validation Loss: 4.912139\n",
      "Epoch: 0147 cost = 5.005144435\n",
      "Validation Loss: 4.9133644\n",
      "Epoch: 0148 cost = 4.998747677\n",
      "Validation Loss: 4.927323\n",
      "Epoch: 0149 cost = 4.994594357\n",
      "Validation Loss: 4.911391\n",
      "Epoch: 0150 cost = 4.997181152\n",
      "Validation Loss: 4.90724\n",
      "Epoch: 0151 cost = 4.991022023\n",
      "Validation Loss: 4.935663\n",
      "Epoch: 0152 cost = 4.996874541\n",
      "Validation Loss: 4.907487\n",
      "Epoch: 0153 cost = 4.988664458\n",
      "Validation Loss: 4.9254766\n",
      "Epoch: 0154 cost = 4.995408467\n",
      "Validation Loss: 4.9257865\n",
      "Epoch: 0155 cost = 4.994174903\n",
      "Validation Loss: 4.9086046\n",
      "Epoch: 0156 cost = 4.989940336\n",
      "Validation Loss: 4.9452076\n",
      "Epoch: 0157 cost = 4.985709067\n",
      "Validation Loss: 4.9176507\n",
      "Epoch: 0158 cost = 4.979076055\n",
      "Validation Loss: 4.8860145\n",
      "Epoch: 0159 cost = 4.984639178\n",
      "Validation Loss: 4.9171424\n",
      "Epoch: 0160 cost = 4.985282248\n",
      "Validation Loss: 4.8877144\n",
      "Epoch: 0161 cost = 4.986120346\n",
      "Validation Loss: 4.890735\n",
      "Epoch: 0162 cost = 4.983142219\n",
      "Validation Loss: 4.909269\n",
      "Epoch: 0163 cost = 4.982739680\n",
      "Validation Loss: 4.913618\n",
      "Epoch: 0164 cost = 4.976919330\n",
      "Validation Loss: 4.902873\n",
      "Epoch: 0165 cost = 4.980370181\n",
      "Validation Loss: 4.946013\n",
      "Epoch: 0166 cost = 4.978016094\n",
      "Validation Loss: 4.913821\n",
      "Epoch: 0167 cost = 4.976665746\n",
      "Validation Loss: 4.9475102\n",
      "Epoch: 0168 cost = 4.977917962\n",
      "Validation Loss: 4.901638\n",
      "Epoch: 0169 cost = 4.969066231\n",
      "Validation Loss: 4.929603\n",
      "Epoch: 0170 cost = 4.967104646\n",
      "Validation Loss: 4.87304\n",
      "Epoch: 0171 cost = 4.962155655\n",
      "Validation Loss: 4.8902373\n",
      "Epoch: 0172 cost = 4.959833931\n",
      "Validation Loss: 4.8990254\n",
      "Epoch: 0173 cost = 4.969156712\n",
      "Validation Loss: 4.8716235\n",
      "Epoch: 0174 cost = 4.968460962\n",
      "Validation Loss: 4.8778524\n",
      "Epoch: 0175 cost = 4.959383418\n",
      "Validation Loss: 4.940509\n",
      "Epoch: 0176 cost = 4.958969541\n",
      "Validation Loss: 4.867774\n",
      "Epoch: 0177 cost = 4.960483980\n",
      "Validation Loss: 4.8643126\n",
      "Epoch: 0178 cost = 4.956779416\n",
      "Validation Loss: 4.9401937\n",
      "Epoch: 0179 cost = 4.958190273\n",
      "Validation Loss: 4.9674873\n",
      "Epoch: 0180 cost = 4.962461287\n",
      "Validation Loss: 4.8933463\n",
      "Epoch: 0181 cost = 4.952031915\n",
      "Validation Loss: 4.9212985\n",
      "Epoch: 0182 cost = 4.951674446\n",
      "Validation Loss: 4.8646264\n",
      "Epoch: 0183 cost = 4.950540826\n",
      "Validation Loss: 4.891623\n",
      "Epoch: 0184 cost = 4.955206649\n",
      "Validation Loss: 4.886452\n",
      "Epoch: 0185 cost = 4.950004813\n",
      "Validation Loss: 4.861957\n",
      "Epoch: 0186 cost = 4.949917538\n",
      "Validation Loss: 4.8624825\n",
      "Epoch: 0187 cost = 4.952226788\n",
      "Validation Loss: 4.8702264\n",
      "Epoch: 0188 cost = 4.950652582\n",
      "Validation Loss: 4.90063\n",
      "Epoch: 0189 cost = 4.953085807\n",
      "Validation Loss: 4.889011\n",
      "Epoch: 0190 cost = 4.947351670\n",
      "Validation Loss: 4.908757\n",
      "Epoch: 0191 cost = 4.941354856\n",
      "Validation Loss: 4.89348\n",
      "Epoch: 0192 cost = 4.952569719\n",
      "Validation Loss: 4.8711596\n",
      "Epoch: 0193 cost = 4.946787395\n",
      "Validation Loss: 4.869918\n",
      "Epoch: 0194 cost = 4.943529759\n",
      "Validation Loss: 4.8676467\n",
      "Epoch: 0195 cost = 4.940796426\n",
      "Validation Loss: 4.9168625\n",
      "Epoch: 0196 cost = 4.940481690\n",
      "Validation Loss: 4.8795004\n",
      "Epoch: 0197 cost = 4.937349051\n",
      "Validation Loss: 4.8639855\n",
      "Epoch: 0198 cost = 4.937644127\n",
      "Validation Loss: 4.9022284\n",
      "Epoch: 0199 cost = 4.931114097\n",
      "Validation Loss: 4.8710213\n",
      "Epoch: 0200 cost = 4.936012409\n",
      "Validation Loss: 4.8646173\n",
      "Optimization Finished!\n",
      "Test Loss: 4.9060307\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    #if a python file, please use the 4 lines bellow and comment the \"n_code = '1'\"\n",
    "    #parser = argparse.ArgumentParser(description='Autoencoder')\n",
    "    #parser.add_argument('n_code', nargs=1, type=str)\n",
    "    #args = parser.parse_args(['--help'])\n",
    "    #n_code = args.n_code[0]\n",
    "    \n",
    "    #if a jupyter file, please comment the 4 above and use the one bellow\n",
    "    n_code = '2'\n",
    "    \n",
    "    #feel free to change with your own \n",
    "    log_files_path = 'C:/Users/Ali/logs/'\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        with tf.variable_scope(\"autoencoder_model\"):\n",
    "\n",
    "            #the input variables are first define as placeholder \n",
    "            # a placeholder is a variable/data which will be assigned later \n",
    "            # image vector & label, phase_train is a boolean \n",
    "            x = tf.placeholder(\"float\", [None, 784]) # MNIST data image of shape 28*28=784\n",
    "            \n",
    "            phase_train = tf.placeholder(tf.bool)\n",
    "            \n",
    "            #define the encoder \n",
    "            code = encoder(x, int(n_code), phase_train)\n",
    "            \n",
    "            #define the decoder\n",
    "            output = decoder(code, int(n_code), phase_train)\n",
    "            \n",
    "            #compute the loss \n",
    "            cost, train_summary_op = loss(output, x)\n",
    "\n",
    "            #initialize the value of the global_step variable \n",
    "            # recall: it is incremented by one each time the .minimise() is called\n",
    "            global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "            train_op = training(cost, global_step)\n",
    "\n",
    "            #evaluate the accuracy of the network (done on a validation set)\n",
    "            eval_op, in_image_op, out_image_op, val_summary_op = evaluate(output, x)\n",
    "\n",
    "            summary_op = tf.summary.merge_all()\n",
    "\n",
    "            #save and restore variables to and from checkpoints.\n",
    "            saver = tf.train.Saver(max_to_keep=200)\n",
    "\n",
    "            #defines a session\n",
    "            sess = tf.Session()\n",
    "\n",
    "            # summary writer\n",
    "            #https://www.tensorflow.org/api_docs/python/tf/summary/FileWriter\n",
    "            train_writer = tf.summary.FileWriter(log_files_path + 'mnist_autoencoder_hidden_' + n_code + '_logs/', graph=sess.graph)\n",
    "\n",
    "            val_writer   = tf.summary.FileWriter(log_files_path + 'mnist_autoencoder_hidden_' + n_code + '_logs/', graph=sess.graph)\n",
    "\n",
    "            #initialization of the variables\n",
    "            init_op = tf.global_variables_initializer()\n",
    "\n",
    "            sess.run(init_op)\n",
    "\n",
    "            # Training cycle\n",
    "            for epoch in range(training_epochs):\n",
    "\n",
    "                avg_cost = 0.\n",
    "                total_batch = int(mnist.train.num_examples/batch_size)\n",
    "                \n",
    "                # Loop over all batches\n",
    "                for i in range(total_batch):\n",
    "                    \n",
    "                    minibatch_x, minibatch_y = mnist.train.next_batch(batch_size)\n",
    "                    \n",
    "                    # Fit training using batch data\n",
    "                    #the training is done using the training dataset\n",
    "                    _, new_cost, train_summary = sess.run([train_op, cost, train_summary_op], feed_dict={x: minibatch_x, phase_train: True})\n",
    "                    \n",
    "                    train_writer.add_summary(train_summary, sess.run(global_step))\n",
    "                    \n",
    "                    # Compute average loss\n",
    "                    avg_cost += new_cost/total_batch\n",
    "                \n",
    "                # Display logs per epoch step\n",
    "                if epoch % display_step == 0:\n",
    "                    print(\"Epoch:\", '%04d' % (epoch+1), \"cost =\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "                    #the accuracy is evaluated using the validation dataset\n",
    "                    train_writer.add_summary(train_summary, sess.run(global_step))\n",
    "\n",
    "                    validation_loss, in_image, out_image, val_summary = sess.run([eval_op, in_image_op, out_image_op, val_summary_op], feed_dict={x: mnist.validation.images, phase_train: False})\n",
    "                    val_writer.add_summary(in_image, sess.run(global_step))\n",
    "                    val_writer.add_summary(out_image, sess.run(global_step))\n",
    "                    val_writer.add_summary(val_summary, sess.run(global_step))\n",
    "                    print(\"Validation Loss:\", validation_loss)\n",
    "\n",
    "                    #save to use later\n",
    "                    #https://www.tensorflow.org/api_docs/python/tf/train/Saver\n",
    "                    #saver.save(sess, log_files_path+'model-checkpoint', global_step=global_step)\n",
    "                    saver.save(sess, log_files_path + 'mnist_autoencoder_hidden_' + n_code + '_logs/model-checkpoint-' + '%04d' % (epoch+1), global_step=global_step)\n",
    "\n",
    "\n",
    "            print(\"Optimization Finished!\")\n",
    "\n",
    "            test_loss = sess.run(eval_op, feed_dict={x: mnist.test.images, phase_train: False})\n",
    "\n",
    "            print(\"Test Loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
